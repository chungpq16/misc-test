{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88da1025",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48786104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai langchain-community tavily-python python-dotenv langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d60614",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úì Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc3111",
   "metadata": {},
   "source": [
    "## 3. Set Up API Keys\n",
    "\n",
    "Required environment variables:\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key\n",
    "- `TAVILY_API_KEY` - Your Tavily API key (get free at https://tavily.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcba79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI API key found\n",
      "‚úì Tavily API key found\n"
     ]
    }
   ],
   "source": [
    "# Check API keys\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  OPENAI_API_KEY not found\")\n",
    "else:\n",
    "    print(\"‚úì OpenAI API key found\")\n",
    "\n",
    "if not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  TAVILY_API_KEY not found\")\n",
    "else:\n",
    "    print(\"‚úì Tavily API key found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01966dd",
   "metadata": {},
   "source": [
    "## 4. Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"‚úì LLM initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5b272",
   "metadata": {},
   "source": [
    "## 5. Set Up Tavily Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tavily search tool initialized\n",
      "  Name: tavily_search_results_json\n",
      "  Description: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tavily search tool\n",
    "search = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    search_depth=\"advanced\"\n",
    ")\n",
    "\n",
    "tools = [search]\n",
    "\n",
    "print(f\"‚úì Tavily search tool initialized\")\n",
    "print(f\"  Name: {search.name}\")\n",
    "print(f\"  Description: {search.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa9a7c",
   "metadata": {},
   "source": [
    "## 6. Bind Tools to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a45235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tools bound to LLM\n"
     ]
    }
   ],
   "source": [
    "# Bind tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"‚úì Tools bound to LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c3bf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langgraph for the official agent\n",
    "!pip install -qU langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72f7d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing ReAct-Style Prompting with Chain\n",
      "================================================================================\n",
      "\n",
      "üß† AGENT THINKING PROCESS:\n",
      "--------------------------------------------------------------------------------\n",
      "Thought: The question is asking for information about the Nobel Prize in Physics for the year 2024, which is beyond my training data. I need to search for the most recent information regarding the Nobel Prize winners.  \n",
      "Action: Search  \n",
      "Action Input: \"Nobel Prize in Physics 2024 winner\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üß† AGENT THINKING PROCESS:\n",
      "--------------------------------------------------------------------------------\n",
      "Thought: The question is asking for information about the Nobel Prize in Physics for the year 2024, which is beyond my training data. I need to search for the most recent information regarding the Nobel Prize winners.  \n",
      "Action: Search  \n",
      "Action Input: \"Nobel Prize in Physics 2024 winner\"\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Testing ReAct-Style Prompting with Chain\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test the chain\n",
    "response = react_chain.invoke({\n",
    "    \"question\": \"Who won the Nobel Prize in Physics in 2024?\"\n",
    "})\n",
    "\n",
    "print(\"\\nüß† AGENT THINKING PROCESS:\")\n",
    "print(\"-\" * 80)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f3b1a",
   "metadata": {},
   "source": [
    "## Test the Official ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae54664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ReAct-style chain created\n",
      "\n",
      "Example usage:\n",
      "response = react_chain.invoke({'question': 'your question here'})\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a simpler ReAct-style chain with explicit prompting\n",
    "react_template = \"\"\"You are a helpful assistant with access to a search tool.\n",
    "\n",
    "Answer the user's question using this EXACT format:\n",
    "\n",
    "Thought: [Explain your reasoning about what you need to do]\n",
    "Action: [Either \"Search\" or \"Answer\"]\n",
    "Action Input: [If Search: your search query | If Answer: your final response]\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let's begin:\"\"\"\n",
    "\n",
    "react_prompt_template = PromptTemplate.from_template(react_template)\n",
    "\n",
    "# Create a simple chain\n",
    "react_chain = react_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úì ReAct-style chain created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"response = react_chain.invoke({'question': 'your question here'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89afe8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = react_chain.invoke({'question': \"What's the difference between LangChain and LlamaIndex?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5f21853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: I need to compare LangChain and LlamaIndex to highlight their differences, focusing on their functionalities, use cases, and any unique features they may have. \\nAction: Search\\nAction Input: \"difference between LangChain and LlamaIndex\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b3415",
   "metadata": {},
   "source": [
    "## 6b. Using LangChain's Built-in ReAct Agent\n",
    "\n",
    "Let's use LangChain's official `create_react_agent` which shows chain of thought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588c868",
   "metadata": {},
   "source": [
    "## 7. Create ReAct Agent with Chain of Thought\n",
    "\n",
    "ReAct = Reasoning + Acting. The agent will explicitly think through each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4758371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ReAct Agent ready with explicit Chain of Thought reasoning\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Create a ReAct prompt that forces the agent to show its thinking\n",
    "react_system_prompt = \"\"\"You are a helpful assistant that uses a search tool when needed.\n",
    "\n",
    "For each query, you MUST follow this format to show your thinking process:\n",
    "\n",
    "Thought: [Your reasoning about what you need to do]\n",
    "Action: [The action you will take - either 'search' or 'respond']\n",
    "Action Input: [If search: the query to search for. If respond: your final answer]\n",
    "\n",
    "After getting search results:\n",
    "Observation: [Summary of what you found]\n",
    "Thought: [Your reasoning about the results]\n",
    "Action: respond\n",
    "Action Input: [Your final answer to the user]\n",
    "\n",
    "IMPORTANT: Always show your Thought process before taking Action!\"\"\"\n",
    "\n",
    "def run_react_agent(query: str):\n",
    "    \"\"\"Run a ReAct agent that shows explicit chain of thought reasoning.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üì• USER QUERY: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Start conversation with system prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=react_system_prompt),\n",
    "        HumanMessage(content=f\"Question: {query}\")\n",
    "    ]\n",
    "    \n",
    "    max_iterations = 5\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"üîÑ AGENT ITERATION {iteration + 1}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get agent's response\n",
    "        response = llm.invoke(messages)\n",
    "        agent_response = response.content\n",
    "        \n",
    "        print(f\"üß† AGENT OUTPUT:\\n{agent_response}\\n\")\n",
    "        \n",
    "        # Parse the agent's response\n",
    "        if \"Action: search\" in agent_response or \"Action: Search\" in agent_response:\n",
    "            # Extract search query\n",
    "            try:\n",
    "                action_input_start = agent_response.find(\"Action Input:\") + len(\"Action Input:\")\n",
    "                search_query = agent_response[action_input_start:].strip().split('\\n')[0].strip()\n",
    "                \n",
    "                print(f\"üîç EXECUTING SEARCH...\")\n",
    "                print(f\"   Query: {search_query}\\n\")\n",
    "                \n",
    "                # Perform search\n",
    "                search_results = search.invoke({\"query\": search_query})\n",
    "                \n",
    "                print(f\"üìä SEARCH RESULTS RECEIVED\")\n",
    "                print(f\"   Found {len(search_results) if isinstance(search_results, list) else 1} result(s)\\n\")\n",
    "                \n",
    "                # Add results to conversation\n",
    "                observation = f\"Observation: Here are the search results:\\n{search_results}\\n\\nNow provide your final answer.\"\n",
    "                messages.append(AIMessage(content=agent_response))\n",
    "                messages.append(HumanMessage(content=observation))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error parsing search: {e}\")\n",
    "                break\n",
    "                \n",
    "        elif \"Action: respond\" in agent_response or \"Action: Respond\" in agent_response:\n",
    "            # Agent is ready to respond\n",
    "            try:\n",
    "                action_input_start = agent_response.find(\"Action Input:\") + len(\"Action Input:\")\n",
    "                final_answer = agent_response[action_input_start:].strip()\n",
    "                \n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"‚úÖ FINAL ANSWER:\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(final_answer)\n",
    "                print()\n",
    "                \n",
    "                return final_answer\n",
    "            except:\n",
    "                # If parsing fails, just return the response\n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"‚úÖ FINAL ANSWER:\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(agent_response)\n",
    "                print()\n",
    "                return agent_response\n",
    "        else:\n",
    "            # No clear action detected, add to messages and continue\n",
    "            messages.append(AIMessage(content=agent_response))\n",
    "    \n",
    "    print(\"‚ö†Ô∏è  Max iterations reached\")\n",
    "    return \"Could not complete the task within iteration limit.\"\n",
    "\n",
    "print(\"‚úì ReAct Agent ready with explicit Chain of Thought reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f993b",
   "metadata": {},
   "source": [
    "## 8. Example 1: Current Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee0621f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì• USER QUERY: What are the latest AI developments in December 2024?\n",
      "================================================================================\n",
      "\n",
      "üîÑ AGENT ITERATION 1\n",
      "--------------------------------------------------------------------------------\n",
      "üß† AGENT OUTPUT:\n",
      "Thought: Since my training only includes data up to October 2023, I need to search for the latest developments in AI specifically for December 2024 to provide accurate information. \n",
      "Action: search\n",
      "Action Input: \"latest AI developments December 2024\"\n",
      "\n",
      "üîç EXECUTING SEARCH...\n",
      "   Query: \"latest AI developments December 2024\"\n",
      "\n",
      "üß† AGENT OUTPUT:\n",
      "Thought: Since my training only includes data up to October 2023, I need to search for the latest developments in AI specifically for December 2024 to provide accurate information. \n",
      "Action: search\n",
      "Action Input: \"latest AI developments December 2024\"\n",
      "\n",
      "üîç EXECUTING SEARCH...\n",
      "   Query: \"latest AI developments December 2024\"\n",
      "\n",
      "üìä SEARCH RESULTS RECEIVED\n",
      "   Found 3 result(s)\n",
      "\n",
      "üîÑ AGENT ITERATION 2\n",
      "--------------------------------------------------------------------------------\n",
      "üìä SEARCH RESULTS RECEIVED\n",
      "   Found 3 result(s)\n",
      "\n",
      "üîÑ AGENT ITERATION 2\n",
      "--------------------------------------------------------------------------------\n",
      "üß† AGENT OUTPUT:\n",
      "Observation: The search results indicate that significant AI developments occurred in December 2024, particularly from Google. Key highlights include the release of Gemini 2.0, which is described as their most capable AI model yet, and the introduction of new models for video and image generation, such as Veo 2 and Imagen 3. Additionally, Google DeepMind launched GenCast, a new AI weather prediction model, and unveiled Willow, a state-of-the-art quantum chip. These advancements reflect a strong focus on generative AI, robotics, and quantum computing.\n",
      "\n",
      "Thought: Based on the information gathered, I can summarize the latest AI developments in December 2024, focusing on the major announcements and innovations from Google.\n",
      "\n",
      "Action: respond\n",
      "Action Input: In December 2024, significant AI developments were announced, particularly by Google. Key updates include the release of Gemini 2.0, their most advanced AI model, and new generative models like Veo 2 and Imagen 3 for video and image creation. Google DeepMind introduced GenCast, an AI weather prediction model, and unveiled Willow, a cutting-edge quantum chip. These advancements highlight a strong emphasis on generative AI, robotics, and quantum computing.\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FINAL ANSWER:\n",
      "================================================================================\n",
      "In December 2024, significant AI developments were announced, particularly by Google. Key updates include the release of Gemini 2.0, their most advanced AI model, and new generative models like Veo 2 and Imagen 3 for video and image creation. Google DeepMind introduced GenCast, an AI weather prediction model, and unveiled Willow, a cutting-edge quantum chip. These advancements highlight a strong emphasis on generative AI, robotics, and quantum computing.\n",
      "\n",
      "üß† AGENT OUTPUT:\n",
      "Observation: The search results indicate that significant AI developments occurred in December 2024, particularly from Google. Key highlights include the release of Gemini 2.0, which is described as their most capable AI model yet, and the introduction of new models for video and image generation, such as Veo 2 and Imagen 3. Additionally, Google DeepMind launched GenCast, a new AI weather prediction model, and unveiled Willow, a state-of-the-art quantum chip. These advancements reflect a strong focus on generative AI, robotics, and quantum computing.\n",
      "\n",
      "Thought: Based on the information gathered, I can summarize the latest AI developments in December 2024, focusing on the major announcements and innovations from Google.\n",
      "\n",
      "Action: respond\n",
      "Action Input: In December 2024, significant AI developments were announced, particularly by Google. Key updates include the release of Gemini 2.0, their most advanced AI model, and new generative models like Veo 2 and Imagen 3 for video and image creation. Google DeepMind introduced GenCast, an AI weather prediction model, and unveiled Willow, a cutting-edge quantum chip. These advancements highlight a strong emphasis on generative AI, robotics, and quantum computing.\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FINAL ANSWER:\n",
      "================================================================================\n",
      "In December 2024, significant AI developments were announced, particularly by Google. Key updates include the release of Gemini 2.0, their most advanced AI model, and new generative models like Veo 2 and Imagen 3 for video and image creation. Google DeepMind introduced GenCast, an AI weather prediction model, and unveiled Willow, a cutting-edge quantum chip. These advancements highlight a strong emphasis on generative AI, robotics, and quantum computing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = run_react_agent(\"What are the latest AI developments in December 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4064ff",
   "metadata": {},
   "source": [
    "## 9. Example 2: Comparison Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d73ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ü§î Processing query: What's the difference between LangChain and LlamaIndex?\n",
      "================================================================================\n",
      "\n",
      "üìç Iteration 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "üîß Tool calls detected: 3\n",
      "\n",
      "  ‚Üí Calling tool: tavily_search_results_json\n",
      "  ‚Üí Arguments: {'query': 'LangChain vs LlamaIndex'}\n",
      "üîß Tool calls detected: 3\n",
      "\n",
      "  ‚Üí Calling tool: tavily_search_results_json\n",
      "  ‚Üí Arguments: {'query': 'LangChain vs LlamaIndex'}\n",
      "  ‚Üí Result: [{'title': 'LangChain vs. LlamaIndex. Main differences - Addepto', 'url': 'https://addepto.com/blog/langchain-vs-llamaindex-main-differences/', 'content': 'Strategic Recommendation: LangChain ecosystem for complex, multi-agent production systems; LlamaIndex for document-centric, retrieval-focused applications [...] model. [...] and text classification.', 'score': 0.9999573}, {'title': 'LlamaIndex vs LangChain: Key Differences, Features & Use Cases', 'url': 'https://www.openxcell.com/blog/llamaindex-vs-langchain/', 'content': 'To Summarize: In the debate of LangChain vs LlamaIndex, the first thing is to understand the fundamentals of both platforms; LangChain‚Äôs modular architecture offers a more flexible workflow, while LlamaIndex‚Äôs quick data indexing capabilities make it ideal for managing large-scale datasets. [...] Let‚Äôs bring your vision to life with our expertise in AI and custom development. Reach out to us to discuss your project today! [...] The difference between LangChain and LlamaIndex is evident even in their introduction. While LlamaIndex is focused on document indexing and retrieval, LangChain offers a more flexible workflow management approach through agent-based decision-making. The two overlap in many ways.\\n\\nSome of the ways that LlamaIndex vs LangChain are similar are:', 'score': 0.99995244}, {'title': \"Llamaindex vs Langchain: What's the difference?\", 'url': 'https://www.ibm.com/think/topics/llamaindex-vs-langchain', 'content': \"# LlamaIndex vs LangChain: What's the difference?\\n\\n## Authors\\n\\nIvan Belcic \\n\\nStaff writer\\n\\nCole Stryker \\n\\nStaff Editor, AI Models\\n\\nIBM Think\\n\\n## LlamaIndex vs LangChain: What's the difference?\\n\\nLlamaIndex and LangChain are two platforms that facilitate the creation and implementation of retrieval-augmented generation (RAG) systems. LlamaIndex is built for streamlined search-and-retrieval, while LangChain is a versatile, modular platform supporting numerous use cases.\", 'score': 0.9999403}]...\n",
      "\n",
      "  ‚Üí Calling tool: tavily_search_results_json\n",
      "  ‚Üí Arguments: {'query': 'LangChain overview'}\n",
      "  ‚Üí Result: [{'title': 'LangChain vs. LlamaIndex. Main differences - Addepto', 'url': 'https://addepto.com/blog/langchain-vs-llamaindex-main-differences/', 'content': 'Strategic Recommendation: LangChain ecosystem for complex, multi-agent production systems; LlamaIndex for document-centric, retrieval-focused applications [...] model. [...] and text classification.', 'score': 0.9999573}, {'title': 'LlamaIndex vs LangChain: Key Differences, Features & Use Cases', 'url': 'https://www.openxcell.com/blog/llamaindex-vs-langchain/', 'content': 'To Summarize: In the debate of LangChain vs LlamaIndex, the first thing is to understand the fundamentals of both platforms; LangChain‚Äôs modular architecture offers a more flexible workflow, while LlamaIndex‚Äôs quick data indexing capabilities make it ideal for managing large-scale datasets. [...] Let‚Äôs bring your vision to life with our expertise in AI and custom development. Reach out to us to discuss your project today! [...] The difference between LangChain and LlamaIndex is evident even in their introduction. While LlamaIndex is focused on document indexing and retrieval, LangChain offers a more flexible workflow management approach through agent-based decision-making. The two overlap in many ways.\\n\\nSome of the ways that LlamaIndex vs LangChain are similar are:', 'score': 0.99995244}, {'title': \"Llamaindex vs Langchain: What's the difference?\", 'url': 'https://www.ibm.com/think/topics/llamaindex-vs-langchain', 'content': \"# LlamaIndex vs LangChain: What's the difference?\\n\\n## Authors\\n\\nIvan Belcic \\n\\nStaff writer\\n\\nCole Stryker \\n\\nStaff Editor, AI Models\\n\\nIBM Think\\n\\n## LlamaIndex vs LangChain: What's the difference?\\n\\nLlamaIndex and LangChain are two platforms that facilitate the creation and implementation of retrieval-augmented generation (RAG) systems. LlamaIndex is built for streamlined search-and-retrieval, while LangChain is a versatile, modular platform supporting numerous use cases.\", 'score': 0.9999403}]...\n",
      "\n",
      "  ‚Üí Calling tool: tavily_search_results_json\n",
      "  ‚Üí Arguments: {'query': 'LangChain overview'}\n",
      "  ‚Üí Result: [{'title': 'Introduction to LangChain - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/', 'content': 'Report\\n\\nLangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for integrating with other tools and end-to-end chains for common applications. It helps AI developers connect LLMs such as GPT-4 with external data and computation. This framework comes for both Python and JavaScript.\\n\\nKey benefits include: [...] The LangChain framework is a great interface to develop interesting AI-powered applications and from personal assistants to prompt management as well as automating tasks. So, keep learning and keep developing powerful applications.', 'score': 0.85538095}, {'title': 'LangChain overview - Docs by LangChain', 'url': 'https://docs.langchain.com/oss/python/langchain/overview', 'content': \"LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework [...] Standard model interface ------------------------ Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in. Learn moreEasy to use, highly flexible agent ---------------------------------- LangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides [...] LangChain overview - Docs by LangChain\\n\\nSkip to main content\\n\\nüöÄ Share how you're building agents for a chance to win LangChain swag!\\n\\nDocs by LangChain home pageImage 1: light logoImage 2: dark logoLangChain + LangGraph\\n\\nSearch...\\n\\nCtrl K\\n\\n   Ask AI\\n   GitHub\\n   Try LangSmith\\n   Try LangSmith\\n\\nSearch...\\n\\nNavigation\\n\\nLangChain overview\\n\\nLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContribute\\n\\nPython\\n\\n   Overview\\n\\n   Changelog\\n\\n##### Get started\\n\\n   Install\\n   Quickstart\\n   Philosophy\", 'score': 0.8512743}, {'title': 'What Is LangChain? Examples and definition | Google Cloud', 'url': 'https://cloud.google.com/use-cases/langchain', 'content': 'What Is LangChain? Examples and definition | Google Cloud\\n\\nPage Contents\\n\\n   Topics\\n\\n   LangChain\\n\\nWhat is LangChain?\\n\\nLangChain is an open-source orchestration framework that simplifies building applications with large language models (LLMs). It provides tools and components to connect LLMs with various data sources, enabling the creation of complex, multi-step workflows. [...] Available as libraries in Python and JavaScript, LangChain helps developers enhance LLM capabilities beyond text generation by linking them to external data and computation. This helps facilitate the development of advanced AI applications like intelligent chatbots, sophisticated question-answering systems, and automated data analysis tools.\\n\\nGet started for free', 'score': 0.8131201}]...\n",
      "\n",
      "  ‚Üí Calling tool: tavily_search_results_json\n",
      "  ‚Üí Arguments: {'query': 'LlamaIndex overview'}\n",
      "  ‚Üí Result: [{'title': 'Introduction to LangChain - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/', 'content': 'Report\\n\\nLangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for integrating with other tools and end-to-end chains for common applications. It helps AI developers connect LLMs such as GPT-4 with external data and computation. This framework comes for both Python and JavaScript.\\n\\nKey benefits include: [...] The LangChain framework is a great interface to develop interesting AI-powered applications and from personal assistants to prompt management as well as automating tasks. So, keep learning and keep developing powerful applications.', 'score': 0.85538095}, {'title': 'LangChain overview - Docs by LangChain', 'url': 'https://docs.langchain.com/oss/python/langchain/overview', 'content': \"LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework [...] Standard model interface ------------------------ Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in. Learn moreEasy to use, highly flexible agent ---------------------------------- LangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides [...] LangChain overview - Docs by LangChain\\n\\nSkip to main content\\n\\nüöÄ Share how you're building agents for a chance to win LangChain swag!\\n\\nDocs by LangChain home pageImage 1: light logoImage 2: dark logoLangChain + LangGraph\\n\\nSearch...\\n\\nCtrl K\\n\\n   Ask AI\\n   GitHub\\n   Try LangSmith\\n   Try LangSmith\\n\\nSearch...\\n\\nNavigation\\n\\nLangChain overview\\n\\nLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContribute\\n\\nPython\\n\\n   Overview\\n\\n   Changelog\\n\\n##### Get started\\n\\n   Install\\n   Quickstart\\n   Philosophy\", 'score': 0.8512743}, {'title': 'What Is LangChain? Examples and definition | Google Cloud', 'url': 'https://cloud.google.com/use-cases/langchain', 'content': 'What Is LangChain? Examples and definition | Google Cloud\\n\\nPage Contents\\n\\n   Topics\\n\\n   LangChain\\n\\nWhat is LangChain?\\n\\nLangChain is an open-source orchestration framework that simplifies building applications with large language models (LLMs). It provides tools and components to connect LLMs with various data sources, enabling the creation of complex, multi-step workflows. [...] Available as libraries in Python and JavaScript, LangChain helps developers enhance LLM capabilities beyond text generation by linking them to external data and computation. This helps facilitate the development of advanced AI applications like intelligent chatbots, sophisticated question-answering systems, and automated data analysis tools.\\n\\nGet started for free', 'score': 0.8131201}]...\n",
      "\n",
      "  ‚Üí Calling tool: tavily_search_results_json\n",
      "  ‚Üí Arguments: {'query': 'LlamaIndex overview'}\n",
      "  ‚Üí Result: [{'title': 'LlamaIndex: An overview - LeewayHertz', 'url': 'https://www.leewayhertz.com/llamaindex/', 'content': 'LlamaIndex, previously known as the GPT Index, is an innovative data framework specially designed to support LLM-based application development. It offers an advanced framework that empowers developers to integrate diverse data sources with large language models. This includes a variety of file formats, such as PDFs and PowerPoints, as well as applications like Notion and Slack and even databases like Postgres and MongoDB. The framework brings an array of connectors that assist in data [...] The Index, or indices, in LlamaIndex, is a data structure that quickly fetches relevant information from external documents based on a user‚Äôs query. It works by dividing documents into text sections known as ‚ÄúNode‚Äù objects and building an index from these pieces. LlamaIndex is foundational for use cases involving the Retrieval Augmented Generation (RAG) of information. In general, indices are built from documents and then used to create Query Engines and Chat Engines. This sets up a [...] LlamaIndex is a powerful and flexible tool specifically designed to enhance the capabilities of large language models. It provides an innovative approach linking varied data sources to LLMs, enabling developers to create more informed and context-rich AI applications.', 'score': 0.9094659}, {'title': 'What is LlamaIndex - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-llamaindex/', 'content': 'LlamaIndex is a flexible, open-source data orchestration framework designed to integrate private, domain-specific data with public data seamlessly for building advanced applications using Large Language Models (LLMs). It simplifies the processes of data ingestion, flexible indexing and intelligent querying hence helping developers and enterprises to create AI applications that are both context-aware and efficient. By adding useful external information to LLMs, it helps to unlock the full [...] ## Key Features of LlamaIndex\\n\\n1. Data Ingestion: LlamaIndex supports connecting to and ingesting data from various sources including APIs, files (PDFs, DOCX), SQL and NoSQL databases, spreadsheets and more. Through LlamaHub, it offers an extensive library of prebuilt connectors to simplify integration, enabling efficient access to both structured and unstructured data. [...] 2. Indexing: A core strength of LlamaIndex is its variety of indexing models, each optimized for different data structures and query needs. These indexing types translate raw data into mathematical representations or structures that facilitate fast, accurate retrieval:\\n\\n List Index: Organizes data sequentially, ideal for working with ordered or evolving datasets like logs or time-series information. It enables straightforward querying where data order matters.', 'score': 0.8565368}, {'title': 'Understanding LlamaIndex: Features and Use Cases', 'url': 'https://bhavikjikadara.medium.com/understanding-llamaindex-features-and-use-cases-c433c3246e86', 'content': 'LlamaIndex is a powerful orchestration framework designed to simplify the integration of private and public data for building applications using Large Language Models (LLMs). By providing tools for data ingestion, indexing, and querying, LlamaIndex enhances the capabilities of LLMs, making them more accessible and versatile for various use cases. [...] ## What is LlamaIndex?\\n\\nLlamaIndex is an orchestration framework that simplifies the integration of private data with public data for building applications using Large Language Models (LLMs). It provides data ingestion, indexing, and querying tools, making it a versatile solution for generative AI needs.\\n\\n## Key features [...] LLMs are capable of ingesting large amounts of unstructured data and returning it in structured formats, and LlamaIndex is set up to make this easy.\\n\\nUsing LlamaIndex, you can get an LLM to read natural language and identify semantically important details such as names, dates, addresses, and figures, and return them in a consistent structured format regardless of the source format.', 'score': 0.85509074}]...\n",
      "\n",
      "üìç Iteration 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Üí Result: [{'title': 'LlamaIndex: An overview - LeewayHertz', 'url': 'https://www.leewayhertz.com/llamaindex/', 'content': 'LlamaIndex, previously known as the GPT Index, is an innovative data framework specially designed to support LLM-based application development. It offers an advanced framework that empowers developers to integrate diverse data sources with large language models. This includes a variety of file formats, such as PDFs and PowerPoints, as well as applications like Notion and Slack and even databases like Postgres and MongoDB. The framework brings an array of connectors that assist in data [...] The Index, or indices, in LlamaIndex, is a data structure that quickly fetches relevant information from external documents based on a user‚Äôs query. It works by dividing documents into text sections known as ‚ÄúNode‚Äù objects and building an index from these pieces. LlamaIndex is foundational for use cases involving the Retrieval Augmented Generation (RAG) of information. In general, indices are built from documents and then used to create Query Engines and Chat Engines. This sets up a [...] LlamaIndex is a powerful and flexible tool specifically designed to enhance the capabilities of large language models. It provides an innovative approach linking varied data sources to LLMs, enabling developers to create more informed and context-rich AI applications.', 'score': 0.9094659}, {'title': 'What is LlamaIndex - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-llamaindex/', 'content': 'LlamaIndex is a flexible, open-source data orchestration framework designed to integrate private, domain-specific data with public data seamlessly for building advanced applications using Large Language Models (LLMs). It simplifies the processes of data ingestion, flexible indexing and intelligent querying hence helping developers and enterprises to create AI applications that are both context-aware and efficient. By adding useful external information to LLMs, it helps to unlock the full [...] ## Key Features of LlamaIndex\\n\\n1. Data Ingestion: LlamaIndex supports connecting to and ingesting data from various sources including APIs, files (PDFs, DOCX), SQL and NoSQL databases, spreadsheets and more. Through LlamaHub, it offers an extensive library of prebuilt connectors to simplify integration, enabling efficient access to both structured and unstructured data. [...] 2. Indexing: A core strength of LlamaIndex is its variety of indexing models, each optimized for different data structures and query needs. These indexing types translate raw data into mathematical representations or structures that facilitate fast, accurate retrieval:\\n\\n List Index: Organizes data sequentially, ideal for working with ordered or evolving datasets like logs or time-series information. It enables straightforward querying where data order matters.', 'score': 0.8565368}, {'title': 'Understanding LlamaIndex: Features and Use Cases', 'url': 'https://bhavikjikadara.medium.com/understanding-llamaindex-features-and-use-cases-c433c3246e86', 'content': 'LlamaIndex is a powerful orchestration framework designed to simplify the integration of private and public data for building applications using Large Language Models (LLMs). By providing tools for data ingestion, indexing, and querying, LlamaIndex enhances the capabilities of LLMs, making them more accessible and versatile for various use cases. [...] ## What is LlamaIndex?\\n\\nLlamaIndex is an orchestration framework that simplifies the integration of private data with public data for building applications using Large Language Models (LLMs). It provides data ingestion, indexing, and querying tools, making it a versatile solution for generative AI needs.\\n\\n## Key features [...] LLMs are capable of ingesting large amounts of unstructured data and returning it in structured formats, and LlamaIndex is set up to make this easy.\\n\\nUsing LlamaIndex, you can get an LLM to read natural language and identify semantically important details such as names, dates, addresses, and figures, and return them in a consistent structured format regardless of the source format.', 'score': 0.85509074}]...\n",
      "\n",
      "üìç Iteration 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ No more tool calls needed\n",
      "\n",
      "================================================================================\n",
      "üìù FINAL ANSWER:\n",
      "================================================================================\n",
      "### Differences Between LangChain and LlamaIndex\n",
      "\n",
      "1. **Purpose and Focus**:\n",
      "   - **LangChain**: This is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a modular architecture that supports complex workflows and agent-based decision-making. LangChain is ideal for building applications that require integration with various tools and data sources, making it suitable for a wide range of use cases, including chatbots and automated data analysis.\n",
      "   - **LlamaIndex**: Previously known as GPT Index, LlamaIndex is focused on document indexing and retrieval. It is designed to facilitate the integration of diverse data sources with LLMs, particularly for applications that require efficient search-and-retrieval capabilities. LlamaIndex is particularly useful for managing large-scale datasets and enhancing the retrieval-augmented generation (RAG) of information.\n",
      "\n",
      "2. **Architecture**:\n",
      "   - **LangChain**: Offers a flexible workflow management approach with a pre-built agent architecture. It allows developers to create complex, multi-step workflows with minimal code, making it user-friendly for rapid development.\n",
      "   - **LlamaIndex**: Utilizes a data structure that quickly fetches relevant information from external documents based on user queries. It organizes data into \"Node\" objects and builds indices from these pieces, which is essential for efficient querying and retrieval.\n",
      "\n",
      "3. **Use Cases**:\n",
      "   - **LangChain**: Best suited for applications that require complex interactions and integrations, such as intelligent chatbots, sophisticated question-answering systems, and automated task management.\n",
      "   - **LlamaIndex**: More focused on applications that need to manage and retrieve information from large datasets, such as document-centric applications and systems that require quick access to indexed data.\n",
      "\n",
      "### Summary\n",
      "- **LangChain** is ideal for developers looking to build complex applications with LLMs that require integration with various tools and workflows.\n",
      "- **LlamaIndex** is tailored for applications that prioritize efficient data retrieval and management, particularly in document-centric contexts.\n",
      "\n",
      "For more detailed comparisons, you can refer to the following resources:\n",
      "- [LangChain vs. LlamaIndex - Addepto](https://addepto.com/blog/langchain-vs-llamaindex-main-differences/)\n",
      "- [LlamaIndex vs LangChain - OpenXcell](https://www.openxcell.com/blog/llamaindex-vs-langchain/)\n",
      "- [LlamaIndex Overview - IBM](https://www.ibm.com/think/topics/llamaindex-vs-langchain)\n",
      "‚úÖ No more tool calls needed\n",
      "\n",
      "================================================================================\n",
      "üìù FINAL ANSWER:\n",
      "================================================================================\n",
      "### Differences Between LangChain and LlamaIndex\n",
      "\n",
      "1. **Purpose and Focus**:\n",
      "   - **LangChain**: This is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a modular architecture that supports complex workflows and agent-based decision-making. LangChain is ideal for building applications that require integration with various tools and data sources, making it suitable for a wide range of use cases, including chatbots and automated data analysis.\n",
      "   - **LlamaIndex**: Previously known as GPT Index, LlamaIndex is focused on document indexing and retrieval. It is designed to facilitate the integration of diverse data sources with LLMs, particularly for applications that require efficient search-and-retrieval capabilities. LlamaIndex is particularly useful for managing large-scale datasets and enhancing the retrieval-augmented generation (RAG) of information.\n",
      "\n",
      "2. **Architecture**:\n",
      "   - **LangChain**: Offers a flexible workflow management approach with a pre-built agent architecture. It allows developers to create complex, multi-step workflows with minimal code, making it user-friendly for rapid development.\n",
      "   - **LlamaIndex**: Utilizes a data structure that quickly fetches relevant information from external documents based on user queries. It organizes data into \"Node\" objects and builds indices from these pieces, which is essential for efficient querying and retrieval.\n",
      "\n",
      "3. **Use Cases**:\n",
      "   - **LangChain**: Best suited for applications that require complex interactions and integrations, such as intelligent chatbots, sophisticated question-answering systems, and automated task management.\n",
      "   - **LlamaIndex**: More focused on applications that need to manage and retrieve information from large datasets, such as document-centric applications and systems that require quick access to indexed data.\n",
      "\n",
      "### Summary\n",
      "- **LangChain** is ideal for developers looking to build complex applications with LLMs that require integration with various tools and workflows.\n",
      "- **LlamaIndex** is tailored for applications that prioritize efficient data retrieval and management, particularly in document-centric contexts.\n",
      "\n",
      "For more detailed comparisons, you can refer to the following resources:\n",
      "- [LangChain vs. LlamaIndex - Addepto](https://addepto.com/blog/langchain-vs-llamaindex-main-differences/)\n",
      "- [LlamaIndex vs LangChain - OpenXcell](https://www.openxcell.com/blog/llamaindex-vs-langchain/)\n",
      "- [LlamaIndex Overview - IBM](https://www.ibm.com/think/topics/llamaindex-vs-langchain)\n"
     ]
    }
   ],
   "source": [
    "result = run_react_agent(\"What's the difference between LangChain and LlamaIndex?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaa0f8",
   "metadata": {},
   "source": [
    "## 10. Example 3: Factual Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4145700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì• USER QUERY: Who won the Nobel Prize in Physics in 2024?\n",
      "================================================================================\n",
      "\n",
      "üîÑ AGENT ITERATION 1\n",
      "--------------------------------------------------------------------------------\n",
      "üß† AGENT OUTPUT:\n",
      "Thought: The question is about the Nobel Prize in Physics for the year 2024, which is beyond my training data that goes up to October 2023. I need to search for the most recent information regarding the Nobel Prize winners for 2024. \n",
      "Action: search\n",
      "Action Input: \"Nobel Prize in Physics 2024 winner\"\n",
      "\n",
      "üîç EXECUTING SEARCH...\n",
      "   Query: \"Nobel Prize in Physics 2024 winner\"\n",
      "\n",
      "üß† AGENT OUTPUT:\n",
      "Thought: The question is about the Nobel Prize in Physics for the year 2024, which is beyond my training data that goes up to October 2023. I need to search for the most recent information regarding the Nobel Prize winners for 2024. \n",
      "Action: search\n",
      "Action Input: \"Nobel Prize in Physics 2024 winner\"\n",
      "\n",
      "üîç EXECUTING SEARCH...\n",
      "   Query: \"Nobel Prize in Physics 2024 winner\"\n",
      "\n",
      "üìä SEARCH RESULTS RECEIVED\n",
      "   Found 3 result(s)\n",
      "\n",
      "üîÑ AGENT ITERATION 2\n",
      "--------------------------------------------------------------------------------\n",
      "üìä SEARCH RESULTS RECEIVED\n",
      "   Found 3 result(s)\n",
      "\n",
      "üîÑ AGENT ITERATION 2\n",
      "--------------------------------------------------------------------------------\n",
      "üß† AGENT OUTPUT:\n",
      "Thought: I found multiple sources confirming the winners of the 2024 Nobel Prize in Physics. Both Geoffrey Hinton and John J. Hopfield were awarded for their contributions to machine learning and artificial neural networks. This information is consistent across the sources, including the official Nobel Prize website. \n",
      "\n",
      "Action: respond\n",
      "Action Input: The Nobel Prize in Physics in 2024 was awarded to Geoffrey Hinton and John J. Hopfield for their foundational discoveries and inventions that enable machine learning with artificial neural networks.\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FINAL ANSWER:\n",
      "================================================================================\n",
      "The Nobel Prize in Physics in 2024 was awarded to Geoffrey Hinton and John J. Hopfield for their foundational discoveries and inventions that enable machine learning with artificial neural networks.\n",
      "\n",
      "üß† AGENT OUTPUT:\n",
      "Thought: I found multiple sources confirming the winners of the 2024 Nobel Prize in Physics. Both Geoffrey Hinton and John J. Hopfield were awarded for their contributions to machine learning and artificial neural networks. This information is consistent across the sources, including the official Nobel Prize website. \n",
      "\n",
      "Action: respond\n",
      "Action Input: The Nobel Prize in Physics in 2024 was awarded to Geoffrey Hinton and John J. Hopfield for their foundational discoveries and inventions that enable machine learning with artificial neural networks.\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FINAL ANSWER:\n",
      "================================================================================\n",
      "The Nobel Prize in Physics in 2024 was awarded to Geoffrey Hinton and John J. Hopfield for their foundational discoveries and inventions that enable machine learning with artificial neural networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = run_react_agent(\"Who won the Nobel Prize in Physics in 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2ac00",
   "metadata": {},
   "source": [
    "## Understanding ReAct Agent's Chain of Thought\n",
    "\n",
    "The agent explicitly shows its reasoning using the **ReAct pattern** (Reasoning + Acting):\n",
    "\n",
    "**Example thinking process:**\n",
    "```\n",
    "Thought: The user is asking about Nobel Prize winners in 2024. \n",
    "         I need to search for the most current information about this.\n",
    "Action: search\n",
    "Action Input: Nobel Prize in Physics 2024 winners\n",
    "\n",
    "Observation: [Search results showing Geoffrey Hinton and John Hopfield won]\n",
    "\n",
    "Thought: Based on the search results, I now have the information needed.\n",
    "         I can provide a complete answer about the winners and their work.\n",
    "Action: respond\n",
    "Action Input: The 2024 Nobel Prize in Physics was awarded to...\n",
    "```\n",
    "\n",
    "This shows the **agent's internal reasoning** at each step - not just what tools it calls, but WHY it's doing what it's doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa06ab8",
   "metadata": {},
   "source": [
    "## Try Your Own Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33f0fa",
   "metadata": {},
   "source": [
    "### Key Features of LangGraph ReAct Agent\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Production-ready**: Built for real applications\n",
    "- ‚úÖ **Streaming support**: Real-time updates\n",
    "- ‚úÖ **Error handling**: Built-in retry logic\n",
    "- ‚úÖ **State management**: Maintains conversation history\n",
    "- ‚úÖ **Extensible**: Easy to add custom logic\n",
    "\n",
    "**Agent Flow:**\n",
    "1. Receives user input\n",
    "2. **Thinks** about what to do\n",
    "3. **Acts** by calling tools when needed\n",
    "4. **Observes** the results\n",
    "5. Repeats until it can answer\n",
    "6. Returns final response\n",
    "\n",
    "This is the **ReAct pattern** (Reasoning + Acting) in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ LANGGRAPH REACT AGENT - ANOTHER EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query = \"What are the latest developments in AI for December 2024?\"\n",
    "print(f\"\\nüì• Question: {query}\\n\")\n",
    "\n",
    "# Simple invoke (non-streaming)\n",
    "result = langgraph_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=query)]}\n",
    ")\n",
    "\n",
    "# Print the conversation\n",
    "print(\"üîÑ AGENT EXECUTION TRACE:\\n\")\n",
    "for msg in result[\"messages\"]:\n",
    "    if msg.type == \"human\":\n",
    "        print(f\"üë§ USER: {msg.content}\\n\")\n",
    "    elif msg.type == \"ai\":\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            print(\"ü§î AGENT DECISION:\")\n",
    "            if msg.content:\n",
    "                print(f\"   {msg.content}\")\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"   ‚Üí Calling: {tc['name']}\")\n",
    "                print(f\"   ‚Üí Args: {tc['args']}\\n\")\n",
    "        else:\n",
    "            print(\"üí¨ AGENT RESPONSE:\")\n",
    "            print(f\"   {msg.content}\\n\")\n",
    "    elif msg.type == \"tool\":\n",
    "        print(f\"üîç TOOL RESULT:\")\n",
    "        result_preview = str(msg.content)[:200] + \"...\" if len(str(msg.content)) > 200 else str(msg.content)\n",
    "        print(f\"   {result_preview}\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574c22e",
   "metadata": {},
   "source": [
    "### Another Example with Different Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ LANGGRAPH REACT AGENT - STREAMING EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query = \"Who won the Nobel Prize in Physics in 2024?\"\n",
    "print(f\"\\nüì• Question: {query}\\n\")\n",
    "\n",
    "# Stream the agent's execution\n",
    "for step in langgraph_agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    # Get the last message in the step\n",
    "    last_message = step[\"messages\"][-1]\n",
    "    \n",
    "    # Pretty print based on message type\n",
    "    if hasattr(last_message, 'type'):\n",
    "        if last_message.type == \"human\":\n",
    "            print(f\"üë§ USER: {last_message.content}\\n\")\n",
    "        \n",
    "        elif last_message.type == \"ai\":\n",
    "            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "                print(\"ü§î AGENT THINKING:\")\n",
    "                if last_message.content:\n",
    "                    print(f\"   Thought: {last_message.content}\")\n",
    "                \n",
    "                for tool_call in last_message.tool_calls:\n",
    "                    print(f\"\\n   üîß Action: Call '{tool_call['name']}'\")\n",
    "                    print(f\"   üìù Input: {tool_call['args']}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(\"‚úÖ FINAL ANSWER:\")\n",
    "                print(f\"   {last_message.content}\\n\")\n",
    "        \n",
    "        elif last_message.type == \"tool\":\n",
    "            print(\"üìä OBSERVATION:\")\n",
    "            # Truncate long results\n",
    "            result = str(last_message.content)\n",
    "            if len(result) > 300:\n",
    "                result = result[:300] + \"...\"\n",
    "            print(f\"   {result}\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57952a",
   "metadata": {},
   "source": [
    "### Run LangGraph Agent with Streaming\n",
    "\n",
    "Stream the agent's thinking process in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c5355",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_react_agent\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create the ReAct agent using LangGraph\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    # Create the ReAct agent using LangGraph\n",
    "    langgraph_agent = create_react_agent(llm, tools)\n",
    "\n",
    "    print(\"‚úì LangGraph ReAct Agent created successfully\")\n",
    "    print(\"\\nThis agent will show its thinking process (ReAct pattern):\")\n",
    "    print(\"  - Thought: What the agent is thinking\")\n",
    "    print(\"  - Action: What tool to call\")\n",
    "    print(\"  - Observation: Results from the tool\")\n",
    "    print(\"  - Final Answer: The response to user\")\n",
    "    \n",
    "except (ImportError, TypeError) as e:\n",
    "    print(f\"‚ö†Ô∏è  LangGraph import error: {e}\")\n",
    "    print(\"\\nAlternative: Using a custom ReAct agent implementation\")\n",
    "    print(\"The custom agent in section 7 works without LangGraph dependencies.\")\n",
    "    \n",
    "    # Create a simple alternative\n",
    "    class SimpleLangGraphAgent:\n",
    "        def __init__(self, llm, tools):\n",
    "            self.llm = llm\n",
    "            self.tools = {tool.name: tool for tool in tools}\n",
    "            \n",
    "        def invoke(self, input_dict):\n",
    "            from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "            messages = input_dict[\"messages\"]\n",
    "            all_messages = []\n",
    "            \n",
    "            for _ in range(5):  # max iterations\n",
    "                response = self.llm.bind_tools(tools).invoke(messages)\n",
    "                all_messages.append(response)\n",
    "                messages.append(response)\n",
    "                \n",
    "                if not response.tool_calls:\n",
    "                    return {\"messages\": all_messages}\n",
    "                \n",
    "                for tool_call in response.tool_calls:\n",
    "                    tool_name = tool_call[\"name\"]\n",
    "                    if tool_name in self.tools:\n",
    "                        result = self.tools[tool_name].invoke(tool_call[\"args\"])\n",
    "                        tool_msg = ToolMessage(\n",
    "                            content=str(result),\n",
    "                            tool_call_id=tool_call[\"id\"]\n",
    "                        )\n",
    "                        all_messages.append(tool_msg)\n",
    "                        messages.append(tool_msg)\n",
    "            \n",
    "            return {\"messages\": all_messages}\n",
    "        \n",
    "        def stream(self, input_dict, stream_mode=\"values\"):\n",
    "            result = self.invoke(input_dict)\n",
    "            for i, msg in enumerate(result[\"messages\"]):\n",
    "                yield {\"messages\": result[\"messages\"][:i+1]}\n",
    "    \n",
    "    langgraph_agent = SimpleLangGraphAgent(llm, tools)\n",
    "    print(\"‚úì Alternative ReAct agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d53d257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.1.25 requires langchain-core<0.3.0,>=0.2.40, but you have langchain-core 1.1.3 which is incompatible.\n",
      "langchain-ollama 0.3.5 requires langchain-core<1.0.0,>=0.3.69, but you have langchain-core 1.1.3 which is incompatible.\n",
      "langchain-text-splitters 0.2.4 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 1.1.3 which is incompatible.\n",
      "langchain-community 0.2.19 requires langchain-core<0.3.0,>=0.2.43, but you have langchain-core 1.1.3 which is incompatible.\n",
      "langchain-community 0.2.19 requires langsmith<0.2.0,>=0.1.112, but you have langsmith 0.4.58 which is incompatible.\n",
      "langchain 0.2.17 requires langchain-core<0.3.0,>=0.2.43, but you have langchain-core 1.1.3 which is incompatible.\n",
      "langchain 0.2.17 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.58 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install LangGraph with compatible versions\n",
    "!pip install -qU \"langgraph>=0.2.0\" \"langchain-core>=0.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969ad12",
   "metadata": {},
   "source": [
    "## LangGraph ReAct Agent Example\n",
    "\n",
    "Using LangGraph's official `create_react_agent` for production-grade agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66748385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom query here\n",
    "my_query = \"What's the weather like in San Francisco today?\"\n",
    "\n",
    "result = run_react_agent(my_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
